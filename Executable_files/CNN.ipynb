{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T19:21:52.902267Z",
     "start_time": "2024-09-25T19:21:45.386575Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # the neural network library of pytorch\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from src.load_dataset import load_mnist, load_fashion_mnist, load_medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # first convolutionnal layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=7,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        # first pooling layer\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2, divisor_override=1)\n",
    "\n",
    "        # second convolutionnal layer\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=7,\n",
    "            out_channels=7,\n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        #second pooling layer\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2, divisor_override=1)\n",
    "\n",
    "        # Flatten\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.fc = nn.Linear(7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flat(x)\n",
    "        output = self.fc(x)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters and training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T19:26:15.940236Z",
     "start_time": "2024-09-25T19:25:59.835811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 990\n",
      "Epoch 0: Loss = 1.076774, accuracy = 65.9000 %\n",
      "Epoch 1: Loss = 0.663204, accuracy = 78.1000 %\n",
      "Epoch 2: Loss = 0.593665, accuracy = 80.7000 %\n",
      "Epoch 3: Loss = 0.548637, accuracy = 83.2500 %\n",
      "Epoch 4: Loss = 0.508211, accuracy = 84.6500 %\n",
      "Epoch 5: Loss = 0.475095, accuracy = 84.6000 %\n",
      "Epoch 6: Loss = 0.448297, accuracy = 85.7000 %\n",
      "Epoch 7: Loss = 0.431784, accuracy = 87.0500 %\n",
      "Epoch 8: Loss = 0.415835, accuracy = 87.1500 %\n",
      "Epoch 9: Loss = 0.401060, accuracy = 87.9000 %\n",
      "Epoch 10: Loss = 0.402729, accuracy = 87.5000 %\n",
      "Epoch 11: Loss = 0.383350, accuracy = 87.8000 %\n",
      "Epoch 12: Loss = 0.368387, accuracy = 89.0500 %\n",
      "Epoch 13: Loss = 0.359229, accuracy = 88.7000 %\n",
      "Epoch 14: Loss = 0.358670, accuracy = 89.5500 %\n",
      "Epoch 15: Loss = 0.348897, accuracy = 89.4500 %\n",
      "Epoch 16: Loss = 0.341985, accuracy = 89.5000 %\n",
      "Epoch 17: Loss = 0.332074, accuracy = 89.8000 %\n",
      "Epoch 18: Loss = 0.326789, accuracy = 89.8000 %\n",
      "Epoch 19: Loss = 0.323423, accuracy = 89.9000 %\n",
      "Epoch 20: Loss = 0.318142, accuracy = 90.1000 %\n",
      "Epoch 21: Loss = 0.312845, accuracy = 90.2000 %\n",
      "Epoch 22: Loss = 0.309306, accuracy = 90.2500 %\n",
      "Epoch 23: Loss = 0.309043, accuracy = 90.4500 %\n",
      "Epoch 24: Loss = 0.301758, accuracy = 91.3500 %\n",
      "Epoch 25: Loss = 0.301060, accuracy = 90.6000 %\n",
      "Epoch 26: Loss = 0.297379, accuracy = 90.9000 %\n",
      "Epoch 27: Loss = 0.293202, accuracy = 91.1000 %\n",
      "Epoch 28: Loss = 0.290217, accuracy = 91.3000 %\n",
      "Epoch 29: Loss = 0.292660, accuracy = 90.7500 %\n",
      "Epoch 30: Loss = 0.286839, accuracy = 91.7000 %\n",
      "Epoch 31: Loss = 0.285413, accuracy = 91.3000 %\n",
      "Epoch 32: Loss = 0.285325, accuracy = 91.6000 %\n",
      "Epoch 33: Loss = 0.284000, accuracy = 91.5500 %\n",
      "Epoch 34: Loss = 0.282746, accuracy = 91.3000 %\n",
      "Epoch 35: Loss = 0.281415, accuracy = 91.5000 %\n",
      "Epoch 36: Loss = 0.280134, accuracy = 91.7500 %\n",
      "Epoch 37: Loss = 0.279323, accuracy = 91.5000 %\n",
      "Epoch 38: Loss = 0.277554, accuracy = 91.7500 %\n",
      "Epoch 39: Loss = 0.277403, accuracy = 91.6500 %\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10  # the number of examples per batch\n",
    "class_set = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # filter dataset\n",
    "I = 16\n",
    "device = torch.device(\"cpu\")\n",
    "network = CNN()\n",
    "learning_rate = 1e-2*(0.66)  # the scale of the changes applied to the weights\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader, test_loader = load_mnist(class_set=class_set, train_dataset_number=2000, test_dataset_number=1000, batch_size=batch_size)\n",
    "# train_loader, test_loader = load_fashion_mnist(class_set=class_set, train_dataset_number=2000, test_dataset_number=1000, batch_size=batch_size)\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "def train_net(network, train_loader, criterion, optimizer):\n",
    "    network.train()  # put in train mode: we will modify the weights of the network\n",
    "    train_loss = 0  # initialize the loss\n",
    "    train_accuracy = 0  # initialize the accuracy\n",
    "\n",
    "    # loop on the batches in the train dataset\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # important step to reset gradients to zero\n",
    "        new_size = I\n",
    "        adaptive_avg_pool = AdaptiveAvgPool2d((new_size, new_size))\n",
    "        data = adaptive_avg_pool(data).to(device)\n",
    "        # Run the network and compute the loss\n",
    "        data = data.sum(dim=1, keepdim=True)\n",
    "        target = target.squeeze()\n",
    "        output = network(data)  # we run the network on the data\n",
    "        loss = criterion(output,\n",
    "                         target)  # we compare output to the target and compute the loss, using the chosen loss function\n",
    "        train_loss += loss.item()  # we increment the total train loss\n",
    "\n",
    "        # !!!Here we do the learning!!!\n",
    "        loss.backward()  # backpropagation: the gradients are automatically computed by the autograd\n",
    "        optimizer.step()  # specific optimization rule for changing the weights (stochastic gradient descent, Adam etc)\n",
    "        # and change weighs\n",
    "\n",
    "        # Getting the prediction of the network and computing the accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # the class chosen by the network is the highest output\n",
    "        acc = pred.eq(target.view_as(pred)).sum().item()  # the accuracy is the proportion of correct classes\n",
    "        train_accuracy += acc  # increment accuracy of whole test set\n",
    "\n",
    "    scheduler.step()\n",
    "    train_accuracy /= len(train_loader.dataset)  # compute mean accuracy\n",
    "    train_loss /= (batch_idx + 1)  # mean loss\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "for epoch in range(40):\n",
    "    train_loss, train_accuracy = train_net(network, train_loader, criterion, optimizer)\n",
    "    loss_list.append(train_loss)\n",
    "    accuracy_list.append(train_accuracy*100)\n",
    "\n",
    "    print(f'Epoch {epoch}: Loss = {train_loss:.6f}, accuracy = {train_accuracy*100:.4f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T19:26:17.342903Z",
     "start_time": "2024-09-25T19:26:17.226559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set: Loss = 0.496585, accuracy = 84.1000 %\n"
     ]
    }
   ],
   "source": [
    "def eval_net(network, test_loader, criterion):\n",
    "    network.eval()  # put in eval mode: we will just run, not modify the network\n",
    "    test_loss = 0  # initialize the loss\n",
    "    test_accuracy = 0  # initialize the accuracy\n",
    "\n",
    "    with torch.no_grad():  # careful, we do not care about gradients here\n",
    "        # loop on the batches in the test dataset\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            new_size = I\n",
    "            adaptive_avg_pool = AdaptiveAvgPool2d((new_size, new_size))\n",
    "            data = adaptive_avg_pool(data).to(device)\n",
    "            # Run the network and compute the loss\n",
    "            data = data.sum(dim=1, keepdim=True)\n",
    "            target = target.squeeze()\n",
    "            output = network(data)  # run the network on the test data\n",
    "            loss = criterion(output,\n",
    "                             target)  # compare the output to the target and compute the loss, using the chosen loss function\n",
    "            test_loss += loss.item()  # increment the total test loss\n",
    "\n",
    "            # Getting the prediction of the network and computing the accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # the class chosen by the network is the highest output\n",
    "            acc = pred.eq(target.view_as(pred)).sum().item()  # the accuracy is the proportion of correct classes\n",
    "            test_accuracy += acc  # increment accuracy of whole test set\n",
    "\n",
    "    test_accuracy /= len(test_loader.dataset)  # compute mean accuracy\n",
    "    test_loss /= (batch_idx + 1)  # mean loss\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "test_loss, test_accuracy = eval_net(network, test_loader, criterion)\n",
    "print(f'Evaluation on test set: Loss = {test_loss:.6f}, accuracy = {test_accuracy*100:.4f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "env3.7",
   "language": "python",
   "display_name": "env3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
